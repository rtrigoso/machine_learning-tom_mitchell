# ğŸ“˜ Chapter 1: Introduction to Machine Learning

---

## 1. What is Machine Learning?

* **Goal:** Programs that improve automatically with experience.

* **Definition (Mitchell, 1997):**

  > A program learns from **experience (E)**, with respect to **tasks (T)** and a **performance measure (P)**, if its performance at T, measured by P, improves with E.

* **Applications:**

  * Speech recognition
  * Fraud detection
  * Self-driving cars
  * Games (checkers, backgammon)
  * Data mining (medical, finance, astronomy)

---

## 2. Well-Posed Learning Problems

* Need to specify:

  1. **Task (T)**
  2. **Performance measure (P)**
  3. **Training experience (E)**

**Example â€“ Checkers**

* T: Playing checkers
* P: % of games won
* E: Self-play games

---

## 3. Designing a Learning System (Checkers Example ğŸ)

### Step 1 â€“ Training Experience

* **Direct feedback** (easier): correct move for each state.
* **Indirect feedback** (harder): only final game outcome (win/lose).
* **Danger:** Training â‰  test distribution â†’ poor generalization.

### Step 2 â€“ Target Function

* Option A: **ChooseMove(board) â†’ best move** (hard).
* Option B: **V(board) â†’ numeric score** (better).

  * Higher = better board state.
  * Choose move â†’ leads to best successor state.

### Step 3 â€“ Representation

* **Linear function of features:**

  ```
  V(b) = w0 + w1x1 + w2x2 + â€¦ + w6x6
  ```

  * x1: # black pieces
  * x2: # red pieces
  * x3: # black kings
  * x4: # red kings
  * x5: black pieces threatened
  * x6: red pieces threatened

ğŸ‘‰ Learning reduces to **finding weights (wâ€™s).**

### Step 4 â€“ Learning Algorithm

* **Need examples (b, Vtrain(b))**.
* Problem: only know final result â†’ estimate intermediate values:

  ```
  Vtrain(b) â‰ˆ V(successor(b))
  ```
* **LMS rule (Least Mean Squares):**

  * For each training example:

    ```
    wi â† wi + Î· (Vtrain(b) â€“ V(b)) xi
    ```

    where Î· = learning rate.
* Equivalent to **stochastic gradient descent**.

---

## 4. Final Design â€“ 4 Modules

```
[ Experiment Generator ] â†’ picks new problems (e.g., self-play)
            â†“
[ Performance System ] â†’ plays checkers using learned V
            â†“
[ Critic ] â†’ evaluates games, produces training examples
            â†“
[ Generalizer ] â†’ learns hypothesis (updates weights)
```

---

## 5. Perspectives on Machine Learning

* **Learning = search** through hypothesis space.
* Different algorithms = different hypothesis representations:

  * Linear functions
  * Decision trees
  * Neural networks
  * Probabilistic models

---

## 6. Key Issues in ML

* Which algorithms converge?
* How much training data is enough?
* How to use prior knowledge?
* How to pick/generate training data?
* How to represent target functions?
* Can representation evolve automatically?

---

## 7. Chapter Summary

* Machine learning = **experience â†’ improved performance**.
* Need: **Task, Performance, Experience**.
* Design involves:

  1. Training experience
  2. Target function
  3. Representation
  4. Algorithm
* Learning = **hypothesis space search**.
* Applications: data mining, adaptation, complex tasks.

---

âœ… **Formulas to Remember**

* **Linear evaluation function:**

  ```
  V(b) = w0 + Î£ wi xi
  ```
* **LMS weight update rule:**

  ```
  wi â† wi + Î· (Vtrain(b) â€“ V(b)) xi
  ```

---

âœ… **Quick Diagram (Learning Loop)**

```
Training Experience â†’ Critic â†’ Training Examples
                                â†“
Experiment Generator â†’ Performance System â†’ Game Trace
                                â†“
                        Generalizer â†’ Hypothesis (f)
```

---

